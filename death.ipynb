{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184b095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training range: 2021-07-15 → 2021-11-14\n",
      "Evaluation range: 2021-11-15 → 2022-01-14\n",
      "Total data range: 2021-07-15 → 2022-01-14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define date ranges for training and evaluation\n",
    "TRAIN_START = '2021-07-15'\n",
    "TRAIN_END = '2021-11-14'\n",
    "EVAL_START = '2021-11-15'\n",
    "EVAL_END = '2022-01-14'\n",
    "\n",
    "print(f\"Training range: {TRAIN_START} → {TRAIN_END}\")\n",
    "print(f\"Evaluation range: {EVAL_START} → {EVAL_END}\")\n",
    "print(f\"Total data range: {TRAIN_START} → {EVAL_END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e90032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading deaths_malaysia.csv...\n",
      "Deaths data shape: (184, 2)\n",
      "Date range: 2021-07-15 00:00:00 to 2022-01-14 00:00:00\n",
      "Sample data:\n",
      "          date  deaths_new\n",
      "485 2021-07-15         110\n",
      "486 2021-07-16         115\n",
      "487 2021-07-17         138\n",
      "488 2021-07-18         153\n",
      "489 2021-07-19         129\n"
     ]
    }
   ],
   "source": [
    "# 1. Load deaths_malaysia.csv (Target variable: deaths_new)\n",
    "print(\"Loading deaths_malaysia.csv...\")\n",
    "deaths_df = pd.read_csv('deaths_malaysia.csv')\n",
    "deaths_df['date'] = pd.to_datetime(deaths_df['date'])\n",
    "\n",
    "# Filter date range\n",
    "deaths_df = deaths_df[(deaths_df['date'] >= TRAIN_START) & (deaths_df['date'] <= EVAL_END)]\n",
    "deaths_df = deaths_df[['date', 'deaths_new']].copy()\n",
    "\n",
    "print(f\"Deaths data shape: {deaths_df.shape}\")\n",
    "print(f\"Date range: {deaths_df['date'].min()} to {deaths_df['date'].max()}\")\n",
    "print(\"Sample data:\")\n",
    "print(deaths_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e2ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cases_malaysia.csv...\n",
      "Cases data shape: (184, 3)\n",
      "Sample data:\n",
      "          date  cases_new  cases_active\n",
      "537 2021-07-15      13215        108370\n",
      "538 2021-07-16      12541        114054\n",
      "539 2021-07-17      12528        119815\n",
      "540 2021-07-18      10710        124594\n",
      "541 2021-07-19      10972        128998\n"
     ]
    }
   ],
   "source": [
    "# 2. Load cases_malaysia.csv (Features: cases_new, cases_active)\n",
    "print(\"Loading cases_malaysia.csv...\")\n",
    "cases_df = pd.read_csv('cases_malaysia.csv')\n",
    "cases_df['date'] = pd.to_datetime(cases_df['date'])\n",
    "\n",
    "# Filter date range and select relevant columns\n",
    "cases_df = cases_df[(cases_df['date'] >= TRAIN_START) & (cases_df['date'] <= EVAL_END)]\n",
    "cases_df = cases_df[['date', 'cases_new', 'cases_active']].copy()\n",
    "\n",
    "print(f\"Cases data shape: {cases_df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(cases_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f149d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icu.csv...\n",
      "ICU data shape: (184, 3)\n",
      "Sample data:\n",
      "        date   icu  icu_vent\n",
      "0 2021-07-15  1206       716\n",
      "1 2021-07-16  1270       709\n",
      "2 2021-07-17  1312       729\n",
      "3 2021-07-18  1317       734\n",
      "4 2021-07-19  1330       737\n"
     ]
    }
   ],
   "source": [
    "# 3. Load icu.csv (Features: icu_covid, vent_covid - ICU and ventilator usage)\n",
    "print(\"Loading icu.csv...\")\n",
    "icu_df = pd.read_csv('icu.csv')\n",
    "icu_df['date'] = pd.to_datetime(icu_df['date'])\n",
    "\n",
    "# Filter date range\n",
    "icu_df = icu_df[(icu_df['date'] >= TRAIN_START) & (icu_df['date'] <= EVAL_END)]\n",
    "\n",
    "# Aggregate by date (sum across all states) and select relevant columns\n",
    "# icu_covid = ICU COVID patients, vent_covid = COVID patients on ventilators\n",
    "icu_agg = icu_df.groupby('date').agg({\n",
    "    'icu_covid': 'sum',\n",
    "    'vent_covid': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "icu_agg.columns = ['date', 'icu', 'icu_vent']\n",
    "\n",
    "print(f\"ICU data shape: {icu_agg.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(icu_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c3a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hospital.csv...\n",
      "Hospital data shape: (184, 3)\n",
      "Sample data:\n",
      "        date   hosp  hosp_new\n",
      "0 2021-07-15   9606      1626\n",
      "1 2021-07-16  10000      1489\n",
      "2 2021-07-17  10080      1768\n",
      "3 2021-07-18  10378      1719\n",
      "4 2021-07-19  11375      1620\n"
     ]
    }
   ],
   "source": [
    "# 4. Load hospital.csv (Features: hosp_covid, admitted_covid - hospitalization data)\n",
    "print(\"Loading hospital.csv...\")\n",
    "hosp_df = pd.read_csv('hospital.csv')\n",
    "hosp_df['date'] = pd.to_datetime(hosp_df['date'])\n",
    "\n",
    "# Filter date range\n",
    "hosp_df = hosp_df[(hosp_df['date'] >= TRAIN_START) & (hosp_df['date'] <= EVAL_END)]\n",
    "\n",
    "# Aggregate by date (sum across all states)\n",
    "# hosp_covid = total COVID hospitalizations, admitted_covid = new COVID admissions\n",
    "hosp_agg = hosp_df.groupby('date').agg({\n",
    "    'hosp_covid': 'sum',\n",
    "    'admitted_covid': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "hosp_agg.columns = ['date', 'hosp', 'hosp_new']\n",
    "\n",
    "print(f\"Hospital data shape: {hosp_agg.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(hosp_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f00e021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tests_malaysia.csv...\n",
      "Tests data shape: (184, 3)\n",
      "Sample data:\n",
      "          date  rtk-ag    pcr\n",
      "538 2021-07-15   77701  74255\n",
      "539 2021-07-16   68659  75090\n",
      "540 2021-07-17   59602  75689\n",
      "541 2021-07-18   50242  59769\n",
      "542 2021-07-19   73269  60676\n"
     ]
    }
   ],
   "source": [
    "# 5. Load tests_malaysia.csv (Features: rtk-ag, pcr - testing intensity)\n",
    "print(\"Loading tests_malaysia.csv...\")\n",
    "tests_df = pd.read_csv('tests_malaysia.csv')\n",
    "tests_df['date'] = pd.to_datetime(tests_df['date'])\n",
    "\n",
    "# Filter date range and select relevant columns\n",
    "tests_df = tests_df[(tests_df['date'] >= TRAIN_START) & (tests_df['date'] <= EVAL_END)]\n",
    "tests_df = tests_df[['date', 'rtk-ag', 'pcr']].copy()\n",
    "\n",
    "print(f\"Tests data shape: {tests_df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(tests_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd2161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vax_malaysia.csv...\n",
      "Vaccination data shape: (184, 5)\n",
      "Sample data:\n",
      "          date  daily_partial  daily_full  daily_booster  cumul_full\n",
      "141 2021-07-15         317950      161424              0     4269859\n",
      "142 2021-07-16         284002      134328              0     4404187\n",
      "143 2021-07-17         264820      132945              0     4537132\n",
      "144 2021-07-18         252620      104252              0     4641384\n",
      "145 2021-07-19         288969      154686              0     4796070\n"
     ]
    }
   ],
   "source": [
    "# 6. Load vax_malaysia.csv (Vaccination features)\n",
    "print(\"Loading vax_malaysia.csv...\")\n",
    "vax_df = pd.read_csv('vax_malaysia.csv')\n",
    "vax_df['date'] = pd.to_datetime(vax_df['date'])\n",
    "\n",
    "# Filter date range and select relevant columns\n",
    "vax_df = vax_df[(vax_df['date'] >= TRAIN_START) & (vax_df['date'] <= EVAL_END)]\n",
    "\n",
    "# Select key vaccination columns\n",
    "vax_columns = ['date', 'daily_partial', 'daily_full', 'daily_booster', 'cumul_full']\n",
    "vax_df = vax_df[vax_columns].copy()\n",
    "\n",
    "print(f\"Vaccination data shape: {vax_df.shape}\")\n",
    "print(\"Sample data:\")\n",
    "print(vax_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e8f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all datasets...\n",
      "  Merging cases data...\n",
      "  Shape after merging cases: (184, 4)\n",
      "  Merging icu data...\n",
      "  Shape after merging icu: (184, 6)\n",
      "  Merging hospital data...\n",
      "  Shape after merging hospital: (184, 8)\n",
      "  Merging tests data...\n",
      "  Shape after merging tests: (184, 10)\n",
      "  Merging vaccination data...\n",
      "  Shape after merging vaccination: (184, 14)\n",
      "\n",
      "Final merged dataset shape: (184, 14)\n",
      "Date range: 2021-07-15 00:00:00 to 2022-01-14 00:00:00\n",
      "\n",
      "Column names:\n",
      "['date', 'deaths_new', 'cases_new', 'cases_active', 'icu', 'icu_vent', 'hosp', 'hosp_new', 'rtk-ag', 'pcr', 'daily_partial', 'daily_full', 'daily_booster', 'cumul_full']\n"
     ]
    }
   ],
   "source": [
    "# 7. Merge all datasets by date\n",
    "print(\"Merging all datasets...\")\n",
    "\n",
    "# Start with deaths as the base (target variable)\n",
    "merged_df = deaths_df.copy()\n",
    "\n",
    "# Merge each dataset\n",
    "datasets_to_merge = [\n",
    "    (cases_df, 'cases'),\n",
    "    (icu_agg, 'icu'),\n",
    "    (hosp_agg, 'hospital'),\n",
    "    (tests_df, 'tests'),\n",
    "    (vax_df, 'vaccination')\n",
    "]\n",
    "\n",
    "for df, name in datasets_to_merge:\n",
    "    print(f\"  Merging {name} data...\")\n",
    "    merged_df = pd.merge(merged_df, df, on='date', how='inner')\n",
    "    print(f\"  Shape after merging {name}: {merged_df.shape}\")\n",
    "\n",
    "print(f\"\\nFinal merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(list(merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b42e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lagged features...\n",
      "Created cases_new_lag7\n",
      "Created cases_new_lag14\n",
      "Created icu_lag7\n",
      "Created cumul_full_lag14\n",
      "Created deaths_new_7d_avg\n",
      "Created cases_new_7d_avg\n",
      "Created hosp_new_7d_avg\n",
      "Created daily_full_7d_avg\n",
      "\n",
      "Dataset shape after feature engineering: (184, 22)\n",
      "New columns added:\n",
      "['cases_new_lag7', 'cases_new_lag14', 'icu_lag7', 'cumul_full_lag14', 'deaths_new_7d_avg', 'cases_new_7d_avg', 'hosp_new_7d_avg', 'daily_full_7d_avg']\n"
     ]
    }
   ],
   "source": [
    "# 8. Create lagged features for better prediction\n",
    "print(\"Creating lagged features...\")\n",
    "\n",
    "# Sort by date to ensure proper lag calculation\n",
    "merged_df = merged_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Define features that should have lags\n",
    "lag_features = {\n",
    "    'cases_new': [7, 14],  # Cases lagged by 1 and 2 weeks\n",
    "    'icu': [7],            # ICU usage lagged by 1 week\n",
    "    'cumul_full': [14]     # Vaccination effect takes ~2 weeks\n",
    "}\n",
    "\n",
    "# Create lagged features\n",
    "for feature, lags in lag_features.items():\n",
    "    for lag in lags:\n",
    "        col_name = f\"{feature}_lag{lag}\"\n",
    "        merged_df[col_name] = merged_df[feature].shift(lag)\n",
    "        print(f\"Created {col_name}\")\n",
    "\n",
    "# Create 7-day rolling averages for daily metrics to smooth noise\n",
    "rolling_features = ['deaths_new', 'cases_new', 'hosp_new', 'daily_full']\n",
    "\n",
    "for feature in rolling_features:\n",
    "    col_name = f\"{feature}_7d_avg\"\n",
    "    merged_df[col_name] = merged_df[feature].rolling(window=7, min_periods=1).mean()\n",
    "    print(f\"Created {col_name}\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {merged_df.shape}\")\n",
    "print(\"New columns added:\")\n",
    "new_cols = [col for col in merged_df.columns if '_lag' in col or '_7d_avg' in col]\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8ec2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/eval sets...\n",
      "Training set shape: (123, 22)\n",
      "Training date range: 2021-07-15 00:00:00 to 2021-11-14 00:00:00\n",
      "Training period: 123 days\n",
      "\n",
      "Evaluation set shape: (61, 22)\n",
      "Evaluation date range: 2021-11-15 00:00:00 to 2022-01-14 00:00:00\n",
      "Evaluation period: 61 days\n",
      "\n",
      "Total dataset: 184 days\n",
      "Train ratio: 66.8%\n",
      "Eval ratio: 33.2%\n"
     ]
    }
   ],
   "source": [
    "# 9. Split data into training and evaluation sets\n",
    "print(\"Splitting data into train/eval sets...\")\n",
    "\n",
    "# Training set: 15 July 2021 → 14 November 2021\n",
    "train_df = merged_df[(merged_df['date'] >= TRAIN_START) & (merged_df['date'] <= TRAIN_END)].copy()\n",
    "\n",
    "# Evaluation set: 15 November 2021 → 14 January 2022  \n",
    "eval_df = merged_df[(merged_df['date'] >= EVAL_START) & (merged_df['date'] <= EVAL_END)].copy()\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Training date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"Training period: {len(train_df)} days\")\n",
    "\n",
    "print(f\"\\nEvaluation set shape: {eval_df.shape}\")\n",
    "print(f\"Evaluation date range: {eval_df['date'].min()} to {eval_df['date'].max()}\")\n",
    "print(f\"Evaluation period: {len(eval_df)} days\")\n",
    "\n",
    "print(f\"\\nTotal dataset: {len(merged_df)} days\")\n",
    "print(f\"Train ratio: {len(train_df)/len(merged_df):.1%}\")\n",
    "print(f\"Eval ratio: {len(eval_df)/len(merged_df):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466aa4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY SUMMARY ===\n",
      "Full dataset shape: (184, 22)\n",
      "Missing values per column:\n",
      "date                  0\n",
      "deaths_new            0\n",
      "cases_new             0\n",
      "cases_active          0\n",
      "icu                   0\n",
      "icu_vent              0\n",
      "hosp                  0\n",
      "hosp_new              0\n",
      "rtk-ag                0\n",
      "pcr                   0\n",
      "daily_partial         0\n",
      "daily_full            0\n",
      "daily_booster         0\n",
      "cumul_full            0\n",
      "cases_new_lag7        7\n",
      "cases_new_lag14      14\n",
      "icu_lag7              7\n",
      "cumul_full_lag14     14\n",
      "deaths_new_7d_avg     0\n",
      "cases_new_7d_avg      0\n",
      "hosp_new_7d_avg       0\n",
      "daily_full_7d_avg     0\n",
      "dtype: int64\n",
      "\n",
      "=== FEATURE OVERVIEW ===\n",
      "\n",
      "Target:\n",
      "  - deaths_new\n",
      "\n",
      "Cases:\n",
      "  - cases_new\n",
      "  - cases_active\n",
      "  - cases_new_lag7\n",
      "  - cases_new_lag14\n",
      "\n",
      "Healthcare Load:\n",
      "  - icu\n",
      "  - icu_vent\n",
      "  - icu_lag7\n",
      "  - hosp\n",
      "  - hosp_new\n",
      "\n",
      "Testing:\n",
      "  - rtk-ag\n",
      "  - pcr\n",
      "\n",
      "Vaccination:\n",
      "  - daily_partial\n",
      "  - daily_full\n",
      "  - daily_booster\n",
      "  - cumul_full\n",
      "  - cumul_full_lag14\n",
      "\n",
      "Rolling Averages:\n",
      "  - deaths_new_7d_avg\n",
      "  - cases_new_7d_avg\n",
      "  - hosp_new_7d_avg\n",
      "  - daily_full_7d_avg\n",
      "\n",
      "=== SAMPLE DATA ===\n",
      "        date  deaths_new  cases_new   icu   hosp  cumul_full\n",
      "0 2021-07-15         110      13215  1206   9606     4269859\n",
      "1 2021-07-16         115      12541  1270  10000     4404187\n",
      "2 2021-07-17         138      12528  1312  10080     4537132\n",
      "3 2021-07-18         153      10710  1317  10378     4641384\n",
      "4 2021-07-19         129      10972  1330  11375     4796070\n",
      "5 2021-07-20          93      12366  1339  10997     4899623\n",
      "6 2021-07-21         199      11985  1332  11255     5070332\n",
      "7 2021-07-22         134      13034  1373  11286     5247316\n",
      "8 2021-07-23         144      15573  1356  11398     5412028\n",
      "9 2021-07-24         184      15902  1399  11783     5549747\n"
     ]
    }
   ],
   "source": [
    "# 10. Data quality check and summary\n",
    "print(\"=== DATA QUALITY SUMMARY ===\")\n",
    "print(f\"Full dataset shape: {merged_df.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "print(\"\\n=== FEATURE OVERVIEW ===\")\n",
    "feature_groups = {\n",
    "    'Target': ['deaths_new'],\n",
    "    'Cases': ['cases_new', 'cases_active', 'cases_new_lag7', 'cases_new_lag14'],\n",
    "    'Healthcare Load': ['icu', 'icu_vent', 'icu_lag7', 'hosp', 'hosp_new'],\n",
    "    'Testing': ['rtk-ag', 'pcr'],\n",
    "    'Vaccination': ['daily_partial', 'daily_full', 'daily_booster', 'cumul_full', 'cumul_full_lag14'],\n",
    "    'Rolling Averages': ['deaths_new_7d_avg', 'cases_new_7d_avg', 'hosp_new_7d_avg', 'daily_full_7d_avg']\n",
    "}\n",
    "\n",
    "for group, features in feature_groups.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    available_features = [f for f in features if f in merged_df.columns]\n",
    "    for f in available_features:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "print(merged_df[['date', 'deaths_new', 'cases_new', 'icu', 'hosp', 'cumul_full']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894c31fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "✅ Saved: processed_covid_data.csv\n",
      "✅ Saved: train_data.csv\n",
      "✅ Saved: eval_data.csv\n",
      "\n",
      "=== SUMMARY ===\n",
      "📊 Total features: 21\n",
      "🎯 Target variable: deaths_new\n",
      "📅 Date range: 2021-07-15 → 2022-01-14\n",
      "🚂 Training: 123 days (2021-07-15 → 2021-11-14)\n",
      "🔍 Evaluation: 61 days (2021-11-15 → 2022-01-14)\n",
      "\n",
      "✨ Ready for machine learning model training!\n",
      "\n",
      "📝 Note: Lagged features have some missing values at the beginning:\n",
      "   - 7-day lags: 7 missing values\n",
      "   - 14-day lags: 14 missing values\n",
      "   These can be handled during model training (drop or impute).\n"
     ]
    }
   ],
   "source": [
    "# 11. Save processed datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Save full merged dataset\n",
    "merged_df.to_csv('processed_covid_data.csv', index=False)\n",
    "print(\"✅ Saved: processed_covid_data.csv\")\n",
    "\n",
    "# Save training set\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "print(\"✅ Saved: train_data.csv\")\n",
    "\n",
    "# Save evaluation set  \n",
    "eval_df.to_csv('eval_data.csv', index=False)\n",
    "print(\"✅ Saved: eval_data.csv\")\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"📊 Total features: {len(merged_df.columns) - 1}\")  # -1 for date column\n",
    "print(f\"🎯 Target variable: deaths_new\")\n",
    "print(f\"📅 Date range: {TRAIN_START} → {EVAL_END}\")\n",
    "print(f\"🚂 Training: {len(train_df)} days ({TRAIN_START} → {TRAIN_END})\")\n",
    "print(f\"🔍 Evaluation: {len(eval_df)} days ({EVAL_START} → {EVAL_END})\")\n",
    "print(f\"\\n✨ Ready for machine learning model training!\")\n",
    "\n",
    "print(f\"\\n📝 Note: Lagged features have some missing values at the beginning:\")\n",
    "print(f\"   - 7-day lags: {7} missing values\")\n",
    "print(f\"   - 14-day lags: {14} missing values\")\n",
    "print(f\"   These can be handled during model training (drop or impute).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
